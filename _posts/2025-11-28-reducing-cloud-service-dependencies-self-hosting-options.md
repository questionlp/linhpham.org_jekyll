---
layout: post
title:  "Reducing Cloud Service Dependencies with Self-Hosting Options"
date:   2025-11-28 10:30:00 -0800
tags:   self-hosting hosting cloud
---

Over the years, I have gone from hosting websites and applications over a DSL connection at home, to hosting them on virtual private servers (VPS), shifting some of the static content hosting to GitHub Pages and made use of services like Cloudflare, Google Analytics, and Google Fonts. In the past year, I have been replacing those services with things that I have more control over on my VPS instances. This is in addition to self-hosting personal instances of Mastodon ([linh.social](https://linh.social/)), Pixelfed ([pxl.linh.social](https://pxl.linh.social/)) and Nextcloud as a means to migrate away from corporate social networks and cloud storage.

I'll cover what and how I have replaced those services to regain more control over my web presence and limit (or even eliminate) the amount of information that gets handed over to large corporations. It also fits with the ethos of a decentralized and independent web and while having the benefit of not being impacted by outages caused by AWS, Cloudflare, and other large cloud service outages.

### Starting with Small Steps

One of the first things I did to take back control of some of my websites was to migrate them from being hosted via GitHub Pages to being served on a small Debian server instance that runs the NGINX web server. Some of the control that I wanted to regain was further customizing the static sites that I generate using either [Jekyll](https://jekyllrb.com/) or [Eleventy](https://www.11ty.dev/), being able to configure redirects and rules in NGINX, and provide a potential out if I chose to migrate most or all of my repositories away from GitHub.

### Dropping Cloudflare

In order to prevent malicious and bot traffic from flooding the Wait Wait Stats Project and other sites that I run, I used Cloudflare's web application firewall, caching and HTTPS tunneling services. The caching services also helped reduce the amount of traffic and load on the web servers for static content such as CSS files and images.

Although the service did help reduce the overall work the web servers that to do to handle the extra traffic, I started to question the need and want to use their services. That was in part due to how much they have inserted themselves into the Internet and in part due to their highly stance on fronting toxic and malicious sites and services.

I made the decision to completely drop Cloudflare without a direct replacement. For SSL/TLS certificates, I used [Certbot](https://certbot.eff.org/) to request and renew certificates from [Let's Encrypt](https://letsencrypt.org/). I created a set of blocking rules for NGINX to block known bad request types rather than passing those requests back to the web applications.

For the Wait Wait Stats Project sites, I stood up an NGINX caching reverse proxy in front of the two web application servers that caches responses generated by the applications and static files (including web fonts, but more on that in a bit). The web application servers were already configured so that NGINX would serve static content directly rather than having the Python applications handling the requests.

If I wanted to reduce the chances of AI bots from pummeling my web servers, I could deploy NGINX rules and `robots.txt` files provided by [ai.robots.txt](https://github.com/ai-robots-txt/ai.robots.txt). The one change that I make to the NGINX rules is to replace the [HTTP 403](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/403) response code to an [HTTP 418](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/418) response code just for fun.

### Hosting Web Fonts

Up until recent versions of the Wait Wait Stats Project sites and several of my personal sites would link to the [IBM Plex](https://www.ibm.com/plex/) web fonts hosted by Google Fonts since it was convenient and I hadn't started using NPM to pull in, bundle and deploy [Bootstrap](https://getbootstrap.com/) files. When I started using building the sites using Bootstrap, I started including the [IBM Plex Mono](https://www.npmjs.com/package/@ibm/plex-mono) and [Plex Sans](https://www.npmjs.com/package/@ibm/plex-sans) NPM packages in my projects and use NPM to deploy the files to the appropriate location.

Making the process easier, the IBM Plex NPM packages include SCSS (Sassy CSS) files that include in my custom Bootstrap SCSS files, along with a variable that sets the appropriate path to the files, so that the appropriate `@font-face` entries are included in the generated and minified CSS files.

For [Bootstrap Icons](https://icons.getbootstrap.com/), I included the NPM package with my project and created a script that copies the required files into the appropriate static files directory and include the `bootstrap-icons.css` file as necessary.

### Replacing Google Analytics

Although I used to use web log analyzers in the past to see what kind of traffic my sites were getting and where they were coming from, I decided to switch to Google Analytics some time ago. While I did appreciate the visibility that they provided, I felt more and more uncomfortable with handing over information over to Google via their tracking cookies and scripts. So, I started to look at alternatives that did not require the use of cookies and could be deployed as a lightweight service.

I eventually landed on [Umami](https://umami.is/) due to the application being fairly lightweight and it does not use any form of tracking cookies. Even though they have free and paid tiers for a hosted version, I only considered the self-hosted option. Umami may not provide all of the features or in-depth information that Google Analytics provides, I rarely used more than a couple of tracking and data analytics features. One feature that both Google Analytics and Umami have is a real-time view of traffic on the sites, which provides me with some insights into why one of my sites was receiving an influx of visitors.

Deploying Umami on to one of my web servers was quite straightfoward as it only required a supported version of Node.js and a MySQL or PostgreSQL database. I installed an LTS version of Node.js and MySQL Server, then followed the steps to build and install the application using the source code. The web application was then proxied by NGINX rather than exposing it directly.

<div class="row">
    <div class="col-6 offset-3">
        <figure class="figure">
            <a target="_blank" href="/assets/images/umami-wait-wait-stats-page.png">
            <img src="/assets/images/umami-wait-wait-stats-page.png" class="img-fluid border" alt="Umami Website Analytics for the Wait Wait Stats Page">
            </a>
            <figcaption class="figure-caption text-center">
                Umami Website Analytics for the Wait Wait Stats Page
            </figcaption>
        </figure>
    </div>
</div>

Once Umami was installed and set up a user, I started adding websites in the application in order to get an ID that I would use when adding the required JavaScript to each of the sites, all while removing references to Google Analytics.

While Umami isn't perfect when it comes to determining the regions where visitors were coming from, especially when it comes to visitors using IPv6, it does provide a high-level view of what pages were being visited, any available referer information, and basic client information such as web browser and operating system. To maintain privacy of visitors, I don't enable logging of IP addresses in website access logs in NGINX for Umami or any of the sites that I host.

The only hitch that I ran into when upgrading from Umami v2 to v3 was migrating the databse from MySQL to PostgreSQL since support for MySQL has been dropped. Their [database migration](https://umami.is/docs/guides/migrate-mysql-postgresql) documentation was incomplete, was not updated to reflect changes made to the application and database structure since it was written, or incorrect information. This lead to significant issues when trying to import data that had quoted or escaped characters, a common occurrence with page titles and location names, via the MySQL database dump and PostgreSQL import steps. Trying to export the data from MySQL as CSV files and importing using PostgreSQL tools was also challenging, due to the number of tables and table names that cause issues with reserved keywords.

Eventually, I had to use JetBrains DataGrip to import data for the problematic tables and ran scripts to replace `\N` with null values where applicable. Thankfully, I had taken multiple backups of the MySQL database and a snapshot of other web server instance before the upgrade and database migration. I later [started a discussion](https://github.com/umami-software/umami/discussions/3708) for their GitHub repository suggesting that they create a tool to do the actual migration of data for those self-hosting the application.

### Next Steps

One service that I still use for my Wait Wait Stats Project is New Relic to monitor the health and performance of the web applications. For now, I am using their free tier and constantly get nagged each time I log in to upgrade to a paid tier, even though I have zero need for it. I have a [Prometheus](https://prometheus.io/) and [Grafana OSS](https://grafana.com/) instance that I run at home, but I haven't had a chance to look at deploying an instance for the public-facing services or other services that can support OpenTelemtry. Hopefully I'll have some free time in the coming months to deep dive into the options and how I would deploy whichever option I decide to use.
